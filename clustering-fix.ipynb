{
 "cells": [
  {
   "cell_type": "raw",
   "id": "74b050ab-0acb-4e21-b5fd-c3be8f239a0d",
   "metadata": {},
   "source": [
    "# SUBSTITITED THE SELFS IN HERE, REMOVED THOSE IN FUNCTIONS\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Clustering class\n",
    "# =============================================================================\n",
    "class Clustering:\n",
    "    \"\"\"\n",
    "    Clustering class for EUCLID.\n",
    "    \n",
    "    This class encapsulates the entire clustering workflow.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adata : sc.AnnData\n",
    "        AnnData object produced by the embedding module.\n",
    "    coordinates : pd.DataFrame\n",
    "        Spatial coordinates for each observation. Columns should include\n",
    "        either ['Section','xccf','yccf','zccf'] or ['Section','z','y','x'].\n",
    "    reconstructed_data_df : pd.DataFrame\n",
    "        Approximated dataset (e.g. from harmonized NMF reconstruction).\n",
    "    standardized_embeddings_GLOBAL : pd.DataFrame\n",
    "        Global standardized embeddings (learned from the reference dataset).\n",
    "    metadata : pd.DataFrame, optional\n",
    "        Additional metadata (e.g., used for merging spatial metadata).\n",
    "    \"\"\"\n",
    "    def __init__(self, adata, coordinates, standardized_embeddings_GLOBAL, metadata):\n",
    "        adata = adata \n",
    "        coordinates = coordinates\n",
    "        reconstructed_data_df = adata.obsm['X_approximated']\n",
    "        standardized_embeddings_GLOBAL = standardized_embeddings_GLOBAL\n",
    "        metadata = metadata\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # BLOCK 1: Conventional Leiden clustering on harmonized NMF embeddings\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a141e36b-0312-4098-a6b9-b10fec1af722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clustering module for EUCLID.\n",
    "This module performs:\n",
    "  - Conventional Leiden clustering on harmonized NMF embeddings.\n",
    "  - A self-supervised, locally enhanced hierarchical (Euclid) clustering.\n",
    "  - Assignment of colors to clusters.\n",
    "  - Application of the learnt clustering tree to new data.\n",
    "  - Anatomical naming of clusters and generation of cluster inspection PDFs.\n",
    "  \n",
    "All functions work on an AnnData object produced by the embedding module.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "from datetime import datetime\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "import backSPIN #############################\n",
    "import leidenalg\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import mannwhitneyu, entropy\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from threadpoolctl import threadpool_limits\n",
    "from tqdm import tqdm\n",
    "from kneed import KneeLocator\n",
    "from PyPDF2 import PdfMerger\n",
    "\n",
    "# Set thread limits and suppress warnings\n",
    "threadpool_limits(limits=8)\n",
    "os.environ['OMP_NUM_THREADS'] = '6'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e27d04-1b3f-4165-b825-988b031dd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Define a Node class for storing the hierarchical clustering tree\n",
    "# =============================================================================\n",
    "class Node:\n",
    "    def __init__(self, level, path=None):\n",
    "        self.level = level\n",
    "        self.path = path if path is not None else []\n",
    "        self.scaler = None\n",
    "        self.nmf = None\n",
    "        self.xgb_model = None\n",
    "        self.feature_importances = None  # feature importances at the split\n",
    "        self.children = {}\n",
    "        self.factors_to_use = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc35949-cc9f-492b-a789-9e1f0c85efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Utility functions (internal)\n",
    "# -------------------------------------------------------------------------\n",
    "def _compute_seeded_NMF(data, gamma_min=0.8, gamma_max=1.5, gamma_num=100):\n",
    "    \"\"\"\n",
    "    Private method to compute seeded NMF (as in embedding) on the given data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        DataFrame of pixels x lipids.\n",
    "    gamma_min : float, optional\n",
    "        Minimum gamma value for Leiden resolution search. (Default is 0.8)\n",
    "    gamma_max : float, optional\n",
    "        Maximum gamma value for Leiden resolution search. (Default is 1.5)\n",
    "    gamma_num : int, optional\n",
    "        Number of gamma values to try. (Default is 100)\n",
    "    Returns\n",
    "    -------\n",
    "    nmfdf : pd.DataFrame\n",
    "        NMF factor matrix (W).\n",
    "    factor_to_lipid : np.ndarray\n",
    "        The H matrix (components x lipids).\n",
    "    N_factors : int\n",
    "        Number of factors.\n",
    "    nmf_model : NMF\n",
    "        Fitted NMF model.\n",
    "    \"\"\"\n",
    "    # 1. Calculate correlation matrix\n",
    "    corr = np.corrcoef(data.values.T)\n",
    "    corr_matrix = np.abs(corr)\n",
    "    np.fill_diagonal(corr_matrix, 0)\n",
    "    # Build dummy AnnData for neighbors\n",
    "    adata_dummy = anndata.AnnData(X=np.zeros_like(corr_matrix))\n",
    "    adata_dummy.obsp['connectivities'] = csr_matrix(corr_matrix)\n",
    "    adata_dummy.uns['neighbors'] = {\n",
    "        'connectivities_key': 'connectivities',\n",
    "        'distances_key': 'distances',\n",
    "        'params': {'n_neighbors': 10, 'method': 'custom'}\n",
    "    }\n",
    "    G = nx.from_numpy_array(corr_matrix)\n",
    "    gamma_values = np.linspace(gamma_min, gamma_max, num=gamma_num) \n",
    "    num_communities = []\n",
    "    modularity_scores = []\n",
    "    objective_values = []\n",
    "    for gamma in gamma_values:\n",
    "        sc.tl.leiden(adata_dummy, resolution=gamma, key_added=f'leiden_{gamma}')\n",
    "        clusters = adata_dummy.obs[f'leiden_{gamma}'].astype(int).values\n",
    "        num_comms = len(np.unique(clusters))\n",
    "        num_communities.append(num_comms)\n",
    "        partition = [np.where(clusters == i)[0] for i in range(num_comms)]\n",
    "        modularity = nx.community.modularity(G, partition)\n",
    "        modularity_scores.append(modularity)\n",
    "    epsilon = 1e-10\n",
    "    alpha = 0.7\n",
    "    for Q, N_c in zip(modularity_scores, num_communities):\n",
    "        f_gamma = Q**alpha * np.log(N_c + 1 + epsilon)\n",
    "        objective_values.append(f_gamma)\n",
    "    max_index = np.argmax(objective_values)\n",
    "    best_gamma = gamma_values[max_index]\n",
    "    best_num_comms = num_communities[max_index]\n",
    "    sc.tl.leiden(adata_dummy, resolution=best_gamma, key_added='leiden_best')\n",
    "    clusters = adata_dummy.obs['leiden_best'].astype(int).values\n",
    "    N_factors = best_num_comms\n",
    "    # 4. Choose representative lipid per cluster\n",
    "    dist = 1 - corr_matrix\n",
    "    np.fill_diagonal(dist, 0)\n",
    "    dist = np.maximum(dist, dist.T)\n",
    "    representatives = []\n",
    "    for i in range(N_factors):\n",
    "        cluster_members = np.where(clusters == i)[0]\n",
    "        if len(cluster_members) > 0:\n",
    "            mean_dist = dist[cluster_members][:, cluster_members].mean(axis=1)\n",
    "            central_idx = cluster_members[np.argmin(mean_dist)]\n",
    "            representatives.append(central_idx)\n",
    "    W_init = data.values[:, representatives]\n",
    "    H_init = corr[representatives, :]\n",
    "    H_init[H_init < 0] = 0.\n",
    "    N_factors = W_init.shape[1]\n",
    "    nmf = NMF(n_components=N_factors, init='custom', random_state=42)\n",
    "    data_offset = data - np.min(data) + 1e-7\n",
    "    data_offset = np.ascontiguousarray(data_offset)\n",
    "    W_init = np.ascontiguousarray(W_init)\n",
    "    H_init = np.ascontiguousarray(H_init)\n",
    "    W = nmf.fit_transform(data_offset, W=W_init, H=H_init)\n",
    "    nmf_result = nmf.transform(data_offset)\n",
    "    nmfdf = pd.DataFrame(nmf_result, index=data.index)\n",
    "    factor_to_lipid = nmf.components_\n",
    "    return nmfdf, factor_to_lipid, N_factors, nmf\n",
    "\n",
    "def _continuity_check(spat, spat_columns=['zccf','yccf','Section'], \n",
    "                       min_val_threshold=10, min_nonzero_sections=3, gaussian_sigma=1.8, default_peak_ratio=10): \n",
    "    \"\"\"\n",
    "    Check whether clusters are continuous along the AP axis.\n",
    "    Parameters\n",
    "    ----------\n",
    "    spat : np.array or DataFrame\n",
    "        Array or DataFrame with spatial coordinates.\n",
    "    spat_columns : list, optional\n",
    "        List of column names to use for the spatial coordinates. (Default is ['zccf','yccf','Section'])\n",
    "    min_val_threshold : int, optional\n",
    "        Minimum value threshold to consider a section count significant. (Default is 10)\n",
    "    min_nonzero_sections : int, optional\n",
    "        Minimum number of sections with nonzero counts required. (Default is 3)\n",
    "    gaussian_sigma : float, optional\n",
    "        Sigma value for Gaussian smoothing. (Default is 1.8)\n",
    "    default_peak_ratio : float, optional\n",
    "        Default peak ratio if insufficient peaks are found. (Default is 10)\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of continuity flags and peak info.\n",
    "    \"\"\"\n",
    "    dd2 = pd.DataFrame(spat, columns=spat_columns)\n",
    "    dd2['Section'] = dd2['Section'].astype(int)\n",
    "    vcnorm = dd2['Section'].astype(int).value_counts()\n",
    "    vcnorm.index = vcnorm.index.astype(int)\n",
    "    vcnorm = vcnorm.sort_index()\n",
    "    enough_sectionss = []\n",
    "    number_of_peakss = []\n",
    "    peak_ratios = []\n",
    "    # For each unique cluster color we expect two values\n",
    "    for cluster in [0, 1]:\n",
    "        test = dd2  # assuming test is subset for a given cluster\n",
    "        value_counts = test['Section'].value_counts().sort_index()\n",
    "        ap = value_counts.values.copy()\n",
    "        ap[ap < min_val_threshold] = 0\n",
    "        ap_nonnull = np.sum(ap > 0) > min_nonzero_sections\n",
    "        apflag = any(ap[i] != 0 and ap[i+1] != 0 for i in range(len(ap)-1))\n",
    "        enough_sections = ap_nonnull and apflag\n",
    "        ap_norm = value_counts / vcnorm.loc[value_counts.index].values\n",
    "        ap_norm = np.array(ap_norm)\n",
    "        zero_padded_ap = np.pad(ap_norm, pad_width=1, mode='constant', constant_values=0)\n",
    "        smoothed_ap = gaussian_filter1d(zero_padded_ap, sigma=gaussian_sigma)\n",
    "        peaks, properties = find_peaks(smoothed_ap, height=0)\n",
    "        number_of_peaks = len(peaks)\n",
    "        if number_of_peaks > 1:\n",
    "            peak_heights = properties['peak_heights']\n",
    "            top_peaks = np.sort(peak_heights)[-2:]\n",
    "            peak_ratio = top_peaks[1] / top_peaks[0]\n",
    "        else:\n",
    "            peak_ratio = default_peak_ratio\n",
    "        enough_sectionss.append(enough_sections)\n",
    "        number_of_peakss.append(number_of_peaks)\n",
    "        peak_ratios.append(peak_ratio)\n",
    "    return enough_sectionss[0], enough_sectionss[1], number_of_peakss[0], number_of_peakss[1], peak_ratios[0], peak_ratios[1]\n",
    "\n",
    "def _differential_lipids(lipidata, kmeans_labels, min_fc=0.2, pthr=0.05):\n",
    "    \"\"\"\n",
    "    Compare two groups (assumed binary) for differential lipids.\n",
    "    Returns the number of altered lipids and a table of promoted ones.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    a = lipidata[kmeans_labels == 0, :]\n",
    "    b = lipidata[kmeans_labels == 1, :]\n",
    "    for rrr in range(lipidata.shape[1]):\n",
    "        groupA = a[:, rrr]\n",
    "        groupB = b[:, rrr]\n",
    "        meanA = np.mean(groupA)\n",
    "        meanB = np.mean(groupB)\n",
    "        log2fold_change = np.abs(np.log2(meanB/meanA)) if meanA > 0 and meanB > 0 else np.nan\n",
    "        try:\n",
    "            _, p_value = mannwhitneyu(groupA, groupB, alternative='two-sided')\n",
    "        except ValueError:\n",
    "            p_value = np.nan\n",
    "        results.append({'lipid': rrr, 'log2fold_change': log2fold_change, 'p_value': p_value})\n",
    "    results_df = pd.DataFrame(results)\n",
    "    reject, pvals_corrected, _, _ = multipletests(results_df['p_value'].values, alpha=0.05, method='fdr_bh')\n",
    "    results_df['p_value_corrected'] = pvals_corrected\n",
    "    promoted = results_df[(results_df['log2fold_change'] > min_fc) & (results_df['p_value_corrected'] < pthr)]\n",
    "    alteredlips = np.sum((results_df['log2fold_change'] > min_fc) & (results_df['p_value_corrected'] < pthr))\n",
    "    return alteredlips, promoted\n",
    "\n",
    "def _rank_features_by_combined_score(tempadata):\n",
    "    \"\"\"\n",
    "    Rank features by combining variance-of-variances and mean variances.\n",
    "    \"\"\"\n",
    "    sections = tempadata.obsm['spatial'][:, 2]\n",
    "    unique_sections = np.unique(sections)\n",
    "    var_of_vars = []\n",
    "    mean_of_vars = []\n",
    "    for i in range(tempadata.X.shape[1]):\n",
    "        feature_values = tempadata.X[:, i]\n",
    "        section_variances = []\n",
    "        for sec in unique_sections:\n",
    "            section_values = feature_values[sections == sec]\n",
    "            section_variance = np.var(section_values)\n",
    "            section_variances.append(section_variance)\n",
    "        var_of_vars.append(np.var(section_variances))\n",
    "        mean_of_vars.append(np.mean(section_variances))\n",
    "    var_of_vars = np.array(var_of_vars) / np.mean(var_of_vars)\n",
    "    mean_of_vars = np.array(mean_of_vars) / np.mean(mean_of_vars)\n",
    "    combined_score = -var_of_vars/2 + mean_of_vars\n",
    "    ranked_indices = np.argsort(combined_score)[::-1]\n",
    "    return ranked_indices\n",
    "\n",
    "def _find_elbow_point(values):\n",
    "    \"\"\"\n",
    "    Find the elbow point in cumulative absolute loadings.\n",
    "    \"\"\"\n",
    "    sorted_values = np.sort(np.abs(values))[::-1]\n",
    "    cumulative_variance = np.cumsum(sorted_values) / np.sum(sorted_values)\n",
    "    kneedle = KneeLocator(range(1, len(cumulative_variance)+1), cumulative_variance, curve='concave', direction='increasing')\n",
    "    elbow = kneedle.elbow\n",
    "    return elbow\n",
    "\n",
    "def _generate_combinations(n, limit=200):\n",
    "    \"\"\"\n",
    "    Generate sorted combinations (of component indices) to try for splitting.\n",
    "    \"\"\"\n",
    "    all_combinations = []\n",
    "    for r in range(n, 0, -1):\n",
    "        for comb in itertools.combinations(range(n), r):\n",
    "            all_combinations.append(comb)\n",
    "            if len(all_combinations) >= limit:\n",
    "                return all_combinations\n",
    "    return all_combinations\n",
    "\n",
    "def _leidenalg_clustering(inputdata, Nneigh=40, Niter=5):\n",
    "    \"\"\"\n",
    "    Faster Leiden clustering using leidenalg.\n",
    "    \"\"\"\n",
    "    nn = NearestNeighbors(n_neighbors=Nneigh, n_jobs=4)\n",
    "    nn.fit(inputdata)\n",
    "    knn = nn.kneighbors_graph(inputdata)\n",
    "    G = nx.Graph(knn)\n",
    "    g = ig.Graph.from_networkx(G)\n",
    "    partitions = leidenalg.find_partition(g, leidenalg.ModularityVertexPartition, n_iterations=Niter, seed=230598)\n",
    "    labels = np.array(partitions.membership)\n",
    "    return labels\n",
    "\n",
    "def _undersample(X, y, sampling_strategy='auto'):\n",
    "    \"\"\"\n",
    "    Under-sample majority class.\n",
    "    \"\"\"\n",
    "    rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "    X_res, y_res = rus.fit_resample(X, y)\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03b880-d560-4147-8753-8331b477f8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2beae0-e614-4a5c-9db2-f47ec7fe1ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85974c84-6995-44a8-b5e4-c68f2a2729e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESTROY THE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29003644-c0f0-43f1-bcd8-044533d04c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=60\n",
    "min_voxels=150\n",
    "min_diff_lipids=2\n",
    "min_fc=0.2\n",
    "pthr=0.05\n",
    "thr_signal=1e-10\n",
    "penalty1=1.5\n",
    "penalty2=2\n",
    "ACCTHR=0.6\n",
    "max_depth=15\n",
    "ds_factor=1\n",
    "spat_columns=['zccf','yccf','Section']\n",
    "min_val_threshold=10\n",
    "min_nonzero_sections=3\n",
    "gaussian_sigma=1.8 \n",
    "default_peak_ratio=10 \n",
    "peak_count_threshold=3 \n",
    "peak_ratio_threshold=1.4 \n",
    "combinations=200\n",
    "xgb_n_estimators=1000\n",
    "xgb_max_depth=8 \n",
    "xgb_learning_rate=0.02  \n",
    "xgb_subsample=0.8 \n",
    "xgb_colsample_bytree=0.8 \n",
    "xgb_gamma=0.5\n",
    "xgb_random_state=42\n",
    "xgb_n_jobs=6 \n",
    "early_stopping_rounds=7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb182c-9202-40ff-a03b-7ce61c600150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "141380c4-21b6-4fde-9de8-8d98f76f0e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 122246 × 151\n",
       "    obs: 'SectionID', 'x', 'y', 'Path', 'Sample', 'Sex', 'Condition', 'Section', 'BadSection', 'xccf', 'yccf', 'zccf', 'x_index', 'y_index', 'z_index', 'boundary', 'acronym', 'id', 'name', 'structure_id_path', 'structure_set_ids', 'rgb_triplet', 'allencolor', 'division', 'SectionPlot'\n",
       "    var: 'old_feature_names'\n",
       "    uns: 'feature_selection_scores'\n",
       "    obsm: 'X_01norm', 'X_Harmonized', 'X_NMF', 'X_TSNE', 'X_approximated'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "\n",
    "adata = ad.read_h5ad(filename='/data/luca/lipidatlas/euclid/euclid_msi/my_adata.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d366214-bbd0-4487-8e94-bdcc53ea82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5cc1c4c-cb58-404a-82ef-d780760662a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zccf</th>\n",
       "      <th>yccf</th>\n",
       "      <th>Section</th>\n",
       "      <th>xccf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>section1_pixel18_104</th>\n",
       "      <td>18</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section1_pixel18_105</th>\n",
       "      <td>18</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section1_pixel18_106</th>\n",
       "      <td>18</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section1_pixel18_110</th>\n",
       "      <td>18</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section1_pixel18_112</th>\n",
       "      <td>18</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section3_pixel229_142</th>\n",
       "      <td>229</td>\n",
       "      <td>142</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section3_pixel229_143</th>\n",
       "      <td>229</td>\n",
       "      <td>143</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section3_pixel229_144</th>\n",
       "      <td>229</td>\n",
       "      <td>144</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section3_pixel229_145</th>\n",
       "      <td>229</td>\n",
       "      <td>145</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section3_pixel229_146</th>\n",
       "      <td>229</td>\n",
       "      <td>146</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122246 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       zccf  yccf  Section  xccf\n",
       "section1_pixel18_104     18   104        1     1\n",
       "section1_pixel18_105     18   105        1     1\n",
       "section1_pixel18_106     18   106        1     1\n",
       "section1_pixel18_110     18   110        1     1\n",
       "section1_pixel18_112     18   112        1     1\n",
       "...                     ...   ...      ...   ...\n",
       "section3_pixel229_142   229   142        3     3\n",
       "section3_pixel229_143   229   143        3     3\n",
       "section3_pixel229_144   229   144        3     3\n",
       "section3_pixel229_145   229   145        3     3\n",
       "section3_pixel229_146   229   146        3     3\n",
       "\n",
       "[122246 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardized_embeddings_GLOBAL = pd.DataFrame(StandardScaler().fit_transform(adata.obsm['X_Harmonized']),\n",
    "                                              index=adata.obs_names)[::DS]\n",
    "metadata = adata.obs.copy()[::DS]\n",
    "coordinates = metadata[['x','y','SectionID', 'SectionID']][::DS]\n",
    "coordinates.columns = [\"zccf\",\"yccf\",\"Section\",\"xccf\"]\n",
    "coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495b76a-80f7-4132-9ad2-daea43d9e104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aab2ea2-e771-45b5-80c5-10f120973c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data_df = adata.obsm['X_approximated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840ec9d-aa15-44db-92db-6d4a625f4078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aeef0e4b-4bbd-4288-ba10-120f2d5600d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=60\n",
    "min_voxels=150\n",
    "min_diff_lipids=2\n",
    "min_fc=0.2\n",
    "pthr=0.05\n",
    "ACCTHR=0.6\n",
    "max_depth=3 ######\n",
    "min_nonzero_sections=1 ######\n",
    "gaussian_sigma=1.8\n",
    "peak_count_threshold=3\n",
    "peak_ratio_threshold=1.4\n",
    "combinations=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b974ed67-c3e9-424b-b9b6-e2f73479605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sections: [1 2 3]\n",
      "Validation sections (valsec): [1]\n",
      "Test sections (testsec): [2]\n",
      "Train sections (trainsec): [3]\n"
     ]
    }
   ],
   "source": [
    "unique_sections = coordinates['Section'].unique()\n",
    "print(\"Unique sections:\", unique_sections)\n",
    "num_sections = len(unique_sections)\n",
    "\n",
    "# Check if we have enough sections for the original splitting approach\n",
    "if num_sections >= 3:\n",
    "    # Initialize empty arrays\n",
    "    valsec = np.array([], dtype=int)\n",
    "    testsec = np.array([], dtype=int)\n",
    "\n",
    "    # Apply the proportional rule, but ensure at least one section in each split\n",
    "    if num_sections >= 5:\n",
    "        valsec = (unique_sections[::5] + 2)[:-1]\n",
    "        testsec = (unique_sections[::5] + 1)[:-1]\n",
    "    else:\n",
    "        # For fewer sections but still >=3, assign at least one to each group\n",
    "        valsec = np.array([unique_sections[0]])\n",
    "        testsec = np.array([unique_sections[1]])\n",
    "\n",
    "    # Double check that validation and test sections are not empty\n",
    "    if len(valsec) == 0:\n",
    "        valsec = np.array([unique_sections[0]])\n",
    "    if len(testsec) == 0:\n",
    "        # Avoid overlap with validation\n",
    "        for sec in unique_sections:\n",
    "            if sec not in valsec:\n",
    "                testsec = np.array([sec])\n",
    "                break\n",
    "\n",
    "    # The rest go to training\n",
    "    trainsec = np.setdiff1d(np.setdiff1d(unique_sections, testsec), valsec)\n",
    "    if len(trainsec) == 0:\n",
    "        if len(valsec) > len(testsec):\n",
    "            trainsec = np.array([valsec[-1]])\n",
    "            valsec = valsec[:-1]\n",
    "        else:\n",
    "            trainsec = np.array([testsec[-1]])\n",
    "            testsec = testsec[:-1]\n",
    "\n",
    "    print(\"Validation sections (valsec):\", valsec)\n",
    "    print(\"Test sections (testsec):\", testsec)\n",
    "    print(\"Train sections (trainsec):\", trainsec)\n",
    "\n",
    "    # Identify point indices for each group\n",
    "    valpoints = coordinates.loc[coordinates['Section'].isin(valsec),:].index\n",
    "    testpoints = coordinates.loc[coordinates['Section'].isin(testsec),:].index\n",
    "    trainpoints = coordinates.loc[coordinates['Section'].isin(trainsec),:].index\n",
    "\n",
    "else:\n",
    "    # Classic 60-20-20 split on the rows (ignoring sections)\n",
    "    print(\"Less than 3 unique sections found. Using 60-20-20 split on rows.\")\n",
    "\n",
    "    # Shuffle indices to ensure random selection\n",
    "    all_indices = coordinates.index.values\n",
    "    np.random.shuffle(all_indices)\n",
    "\n",
    "    # Calculate split sizes\n",
    "    n_samples = len(all_indices)\n",
    "    n_train = int(0.6 * n_samples)\n",
    "    n_val = int(0.2 * n_samples)\n",
    "\n",
    "    # Split indices\n",
    "    trainpoints = all_indices[:n_train]\n",
    "    valpoints = all_indices[n_train:n_train+n_val]\n",
    "    testpoints = all_indices[n_train+n_val:]\n",
    "\n",
    "    # For consistency with the section-based approach\n",
    "    trainsec = np.array([])\n",
    "    valsec = np.array([])\n",
    "    testsec = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68b2a729-6522-4265-b17b-3a9a174d2f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (122246, 151)\n",
      "Raw lipids data shape: (122246, 151)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for clustering\n",
    "data = pd.DataFrame(reconstructed_data_df.copy(), index=standardized_embeddings_GLOBAL.index)\n",
    "print(\"Data shape:\", data.shape)\n",
    "rawlips = data.copy()\n",
    "print(\"Raw lipids data shape:\", rawlips.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e11d4151-7c77-4832-9d10-7c7f725c4824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd percentile values:\n",
      " 0      0.004364\n",
      "1      0.004416\n",
      "2      0.005842\n",
      "3      0.004443\n",
      "4      0.005088\n",
      "         ...   \n",
      "146    0.004375\n",
      "147    0.004620\n",
      "148    0.004348\n",
      "149    0.004357\n",
      "150    0.004332\n",
      "Name: 0.02, Length: 151, dtype: float64\n",
      "98th percentile values:\n",
      " 0      0.004405\n",
      "1      0.004563\n",
      "2      0.009400\n",
      "3      0.004828\n",
      "4      0.006201\n",
      "         ...   \n",
      "146    0.004590\n",
      "147    0.005765\n",
      "148    0.004569\n",
      "149    0.004740\n",
      "150    0.005182\n",
      "Name: 0.98, Length: 151, dtype: float64\n",
      "Normalized values shape: (122246, 151)\n",
      "Normalized and clipped data shape: (122246, 151)\n"
     ]
    }
   ],
   "source": [
    "# Normalize raw data using percentiles (2% and 98%)\n",
    "p2 = rawlips.quantile(0.02)\n",
    "p98 = rawlips.quantile(0.98)\n",
    "print(\"2nd percentile values:\\n\", p2)\n",
    "print(\"98th percentile values:\\n\", p98)\n",
    "normalized_values = (rawlips.values - p2.values) / (p98.values - p2.values)\n",
    "print(\"Normalized values shape:\", normalized_values.shape)\n",
    "clipped_values = np.clip(normalized_values, 0, 1)\n",
    "normalized_datemp = pd.DataFrame(clipped_values, columns=rawlips.columns, index=rawlips.index)\n",
    "print(\"Normalized and clipped data shape:\", normalized_datemp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d208214b-2dbd-4923-9cf3-e926de298e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created AnnData with shape: (122246, 151)\n",
      "Sample spatial coordinates:\n",
      " [[ 18 104   1]\n",
      " [ 18 105   1]\n",
      " [ 18 106   1]\n",
      " [ 18 110   1]\n",
      " [ 18 112   1]]\n",
      "Sample lipid data:\n",
      "                            0         1         2         3         4    \\\n",
      "section1_pixel18_104  0.411354  0.684223  1.000000  0.322268  0.828163   \n",
      "section1_pixel18_105  0.097681  0.634793  0.761229  0.422297  0.468835   \n",
      "section1_pixel18_106  0.105777  0.601587  0.856403  0.490948  0.554111   \n",
      "section1_pixel18_110  0.205442  0.569963  0.751861  0.259524  0.548884   \n",
      "section1_pixel18_112  0.199275  0.719065  0.810193  0.456313  0.536024   \n",
      "\n",
      "                           5         6    7         8         9    ...  141  \\\n",
      "section1_pixel18_104  0.716551  0.013703  0.0  0.721167  0.562022  ...  0.0   \n",
      "section1_pixel18_105  0.342687  0.000000  0.0  0.886240  0.606591  ...  0.0   \n",
      "section1_pixel18_106  0.423469  0.000000  0.0  1.000000  0.646003  ...  0.0   \n",
      "section1_pixel18_110  0.371520  0.000000  0.0  0.452979  0.541941  ...  0.0   \n",
      "section1_pixel18_112  0.381497  0.014621  0.0  0.904116  0.647617  ...  0.0   \n",
      "\n",
      "                           142       143       144       145       146  \\\n",
      "section1_pixel18_104  0.956386  0.900565  0.613259  0.467054  1.000000   \n",
      "section1_pixel18_105  1.000000  0.727189  0.812433  0.676704  0.987336   \n",
      "section1_pixel18_106  1.000000  0.821018  0.851896  0.734736  0.989213   \n",
      "section1_pixel18_110  0.801279  0.795437  0.545298  0.364429  0.827021   \n",
      "section1_pixel18_112  1.000000  0.823040  0.817253  0.636058  0.917484   \n",
      "\n",
      "                           147  148  149  150  \n",
      "section1_pixel18_104  0.824622  0.0  0.0  0.0  \n",
      "section1_pixel18_105  0.737297  0.0  0.0  0.0  \n",
      "section1_pixel18_106  0.750510  0.0  0.0  0.0  \n",
      "section1_pixel18_110  0.659680  0.0  0.0  0.0  \n",
      "section1_pixel18_112  0.754593  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 151 columns]\n"
     ]
    }
   ],
   "source": [
    "# Prepare a Scanpy object with raw lipids for differential testing.\n",
    "adata = sc.AnnData(X=data)\n",
    "adata.obsm['spatial'] = coordinates[['zccf','yccf','Section']].loc[data.index].values\n",
    "adata.obsm['lipids'] = normalized_datemp\n",
    "print(\"Created AnnData with shape:\", adata.shape)\n",
    "print(\"Sample spatial coordinates:\\n\", adata.obsm['spatial'][:5])\n",
    "print(\"Sample lipid data:\\n\", normalized_datemp.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab9db970-5e68-4b61-85a3-2e5c6221eb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized clustering log with shape: (122246, 15)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a log DataFrame for clustering history.\n",
    "column_names = [f\"level_{i}\" for i in range(1, max_depth+1)]\n",
    "clusteringLOG = pd.DataFrame(0, index=data.index, columns=column_names)[::ds_factor]\n",
    "print(\"Initialized clustering log with shape:\", clusteringLOG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2344f-1ae7-4bb1-be5b-9c1f201da8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618dc0a3-c0ab-4959-a3a3-1154ca2c411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the recursive splitting function.\n",
    "def _dosplit(current_adata, embds, path=[], splitlevel=0):\n",
    "    print(\"\\n=== Entering _dosplit at level:\", splitlevel, \"with\", current_adata.X.shape[0], \"voxels ===\")\n",
    "    if current_adata.X.shape[0] < min_voxels:\n",
    "        print(\"Branch exhausted due to low voxel count:\", current_adata.X.shape[0])\n",
    "        return None\n",
    "\n",
    "    # Compute a local NMF on current data\n",
    "    nmfdf, loadings, N_factors, nmf_model = _compute_seeded_NMF(pd.DataFrame(current_adata.X, index=current_adata.obs_names))\n",
    "    nmf_result = nmfdf.values\n",
    "    print(\"Computed NMF. nmfdf shape:\", nmfdf.shape, \"N_factors:\", N_factors)\n",
    "    print(\"Loadings shape:\", loadings.shape)\n",
    "    print(\"Mean absolute values per factor:\", np.abs(nmf_result).mean(axis=0))\n",
    "\n",
    "    filter1 = np.abs(nmf_result).mean(axis=0) > thr_signal\n",
    "    loadings_sel = loadings[filter1, :]\n",
    "    nmf_result = nmf_result[:, filter1]\n",
    "    original_nmf_indices = np.arange(N_factors)[filter1]\n",
    "    print(\"Selected\", filter1.sum(), \"factors after filtering out of\", len(filter1))\n",
    "    print(\"Shape of filtered nmf_result:\", nmf_result.shape)\n",
    "\n",
    "    tempadata = sc.AnnData(X=nmf_result)\n",
    "    tempadata.obsm['spatial'] = current_adata.obsm['spatial']\n",
    "\n",
    "    # Rank features by combined score\n",
    "    goodpcs = _rank_features_by_combined_score(tempadata)\n",
    "    print(\"Ranked features (goodpcs):\", goodpcs)\n",
    "    goodpcs_indices = original_nmf_indices[goodpcs.astype(int)]\n",
    "    top_pcs_data = nmf_result[:, goodpcs.astype(int)]\n",
    "    loadings_sel = loadings_sel[goodpcs.astype(int), :]\n",
    "    print(\"Top PCs data shape:\", top_pcs_data.shape)\n",
    "    print(\"Indices of good principal components:\", goodpcs_indices)\n",
    "\n",
    "    multiplets = _generate_combinations(len(goodpcs), limit=combinations)\n",
    "    print(\"Generated multiplets count:\", len(multiplets))\n",
    "    flag = False\n",
    "    aaa = 0\n",
    "\n",
    "    # Begin iterative search for acceptable split\n",
    "    while (not flag) and (aaa < len(multiplets)):\n",
    "        bestpcs = multiplets[aaa]\n",
    "        print(\"\\nIteration\", aaa, \"using bestpcs indices:\", bestpcs)\n",
    "        embeddings_local = top_pcs_data[:, bestpcs]\n",
    "        loadings_current = loadings_sel[list(bestpcs), :]\n",
    "        selected_nmf_indices = goodpcs_indices[list(bestpcs)]\n",
    "        scaler_local = StandardScaler()\n",
    "        standardized_embeddings = scaler_local.fit_transform(embeddings_local)\n",
    "        print(\"Standardized embeddings shape:\", standardized_embeddings.shape)\n",
    "\n",
    "        # Combine with previous split and global embeddings\n",
    "        globembds = standardized_embeddings_GLOBAL.loc[current_adata.obs_names].values / penalty2\n",
    "        embspace = np.concatenate((standardized_embeddings, embds/penalty1, globembds), axis=1)\n",
    "        print(\"Combined embedding space shape:\", embspace.shape)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=K, random_state=230598)\n",
    "        kmeans_labels = kmeans.fit_predict(embspace)\n",
    "        print(\"KMeans labels distribution:\", np.bincount(kmeans_labels))\n",
    "\n",
    "        # Reaggregate via backSPIN (using its API)\n",
    "        data_for_clustering = pd.DataFrame(current_adata.X, index=current_adata.obs_names, columns=current_adata.var_names)\n",
    "        data_for_clustering['label'] = kmeans_labels\n",
    "        centroids = data_for_clustering.groupby('label').mean()\n",
    "        centroids = pd.DataFrame(StandardScaler().fit_transform(centroids), columns=centroids.columns, index=centroids.index).T\n",
    "        print(\"Centroids shape after standardization and transpose:\", centroids.shape)\n",
    "        row_ix, columns_ix = backSPIN.SPIN(centroids, widlist=4)\n",
    "        centroids = centroids.iloc[row_ix, columns_ix]\n",
    "        print(\"Centroids shape after backSPIN reordering:\", centroids.shape)\n",
    "        _, _, _, gr1, gr2, _, _, _, _ = backSPIN._divide_to_2and_resort(sorted_data=centroids.values, wid=5)\n",
    "        gr1 = np.array(centroids.columns)[gr1]\n",
    "        gr2 = np.array(centroids.columns)[gr2]\n",
    "        print(\"Division groups sizes: gr1 =\", len(gr1), \"gr2 =\", len(gr2))\n",
    "        data_for_clustering['lab'] = 1\n",
    "        data_for_clustering.loc[data_for_clustering['label'].isin(gr2), 'lab'] = 2\n",
    "\n",
    "        # Check continuity along AP axis using coordinates and differential lipids in adata.obsm['lipids']\n",
    "        enough_sections0, enough_sections1, num_peaks0, num_peaks1, peak_ratio0, peak_ratio1 = _continuity_check(\n",
    "            current_adata.obsm['spatial'], \n",
    "            spat_columns=spat_columns, \n",
    "            min_val_threshold=min_val_threshold,\n",
    "            min_nonzero_sections=min_nonzero_sections, \n",
    "            gaussian_sigma=gaussian_sigma, \n",
    "            default_peak_ratio=default_peak_ratio \n",
    "        )\n",
    "        print(\"Continuity check results:\",\n",
    "              \"enough_sections0 =\", enough_sections0,\n",
    "              \"enough_sections1 =\", enough_sections1,\n",
    "              \"num_peaks0 =\", num_peaks0,\n",
    "              \"num_peaks1 =\", num_peaks1,\n",
    "              \"peak_ratio0 =\", peak_ratio0,\n",
    "              \"peak_ratio1 =\", peak_ratio1)\n",
    "\n",
    "        alteredlips, promoted = _differential_lipids(current_adata.obsm['lipids'].values, kmeans_labels, min_fc, pthr)\n",
    "        print(\"Differential lipids count:\", alteredlips, \"Promoted:\", promoted)\n",
    "\n",
    "        flag = ((np.sum(kmeans_labels == 1) > min_voxels or np.sum(kmeans_labels == 0) > min_voxels)\n",
    "                and (alteredlips > min_diff_lipids)\n",
    "                and enough_sections0 and enough_sections1\n",
    "                and ((num_peaks0 < peak_count_threshold) or (peak_ratio0 > peak_ratio_threshold)) \n",
    "                and ((num_peaks1 < peak_count_threshold) or (peak_ratio1 > peak_ratio_threshold)))\n",
    "\n",
    "        print(np.sum(kmeans_labels == 1) > min_voxels)\n",
    "        print(np.sum(kmeans_labels == 0) > min_voxels)\n",
    "        print(alteredlips > min_diff_lipids)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Flag condition evaluated to:\", flag)\n",
    "        aaa += 1\n",
    "        kmeans_labels = data_for_clustering['lab'].astype(int)\n",
    "\n",
    "    if not flag:\n",
    "        print(\"Branch exhausted due to failure of continuity or differential criteria.\")\n",
    "        return None\n",
    "\n",
    "    # Train an XGB classifier on the embeddings\n",
    "    embeddings_df = pd.DataFrame(embspace, index=current_adata.obs_names)\n",
    "    print(\"Embeddings dataframe shape:\", embeddings_df.shape)\n",
    "\n",
    "    X_train = embeddings_df.loc[embeddings_df.index.isin(trainpoints), :]\n",
    "    X_val = embeddings_df.loc[embeddings_df.index.isin(valpoints), :]\n",
    "    X_test = embeddings_df.loc[embeddings_df.index.isin(testpoints), :]\n",
    "    print(\"Training set shape:\", X_train.shape)\n",
    "    print(\"Validation set shape:\", X_val.shape)\n",
    "    print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "    kmeans_labels = kmeans_labels - 1\n",
    "    y_train = kmeans_labels.loc[X_train.index]\n",
    "    y_val = kmeans_labels.loc[X_val.index]\n",
    "    y_test = kmeans_labels.loc[X_test.index]\n",
    "\n",
    "    X_train_sub, y_train_sub = _undersample(X_train, y_train)\n",
    "    print(\"After undersampling, training set shape:\", X_train_sub.shape)\n",
    "\n",
    "    xgb_model = XGBClassifier(  \n",
    "        n_estimators=xgb_n_estimators, \n",
    "        max_depth=xgb_max_depth, \n",
    "        learning_rate=xgb_learning_rate,\n",
    "        subsample=xgb_subsample, \n",
    "        colsample_bytree=xgb_colsample_bytree, \n",
    "        gamma=xgb_gamma, \n",
    "        random_state=xgb_random_state,  \n",
    "        n_jobs=xgb_n_jobs  \n",
    "    )\n",
    "    print(\"Training XGB classifier...\")\n",
    "    xgb_model.fit(\n",
    "        X_train_sub,\n",
    "        y_train_sub,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        # callbacks=[xgb.callback.EarlyStopping(rounds=early_stopping_rounds)],  #### CURRENTLY FROZEN DUE TO PACKAGE INCOMPATIBILITIES THAT ARE NON TRIVIAL TO FIX.\n",
    "        verbose=False\n",
    "    )\n",
    "    test_pred = xgb_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "    if test_acc < ACCTHR:\n",
    "        print(\"Branch exhausted due to poor classifier generalization.\")\n",
    "        return None\n",
    "\n",
    "    # Overwrite cluster labels with classifier predictions (for consistency)\n",
    "    new_labels = pd.concat([pd.Series(xgb_model.predict(X_train), index=X_train.index),\n",
    "                             pd.Series(xgb_model.predict(X_val), index=X_val.index),\n",
    "                             pd.Series(xgb_model.predict(X_test), index=X_test.index)])\n",
    "    new_labels = new_labels.loc[embeddings_df.index]\n",
    "    new_labels = new_labels + 1  # adjust if needed\n",
    "    print(\"New labels distribution:\\n\", new_labels.value_counts())\n",
    "\n",
    "    # Update clustering log\n",
    "    clusteringLOG.loc[new_labels.index, f\"level_{splitlevel+1}\"] = new_labels.values\n",
    "    print(\"Updated clustering log for level\", splitlevel+1)\n",
    "\n",
    "    # Create a Node for this split\n",
    "    node = Node(splitlevel, path=path)\n",
    "    node.scaler = scaler_local\n",
    "    node.nmf = nmf_model\n",
    "    node.xgb_model = xgb_model\n",
    "    node.feature_importances = xgb_model.feature_importances_\n",
    "    node.factors_to_use = selected_nmf_indices\n",
    "    print(\"Created node at level\", splitlevel, \"with factors:\", selected_nmf_indices)\n",
    "\n",
    "    # Recursively split the two branches\n",
    "    idx0 = embeddings_df.index[new_labels == 1]\n",
    "    idx1 = embeddings_df.index[new_labels == 2]\n",
    "    print(\"Branch indices - group 1:\", idx0, \"\\nBranch indices - group 2:\", idx1)\n",
    "    adata0 = current_adata[current_adata.obs_names.isin(idx0)]\n",
    "    adata1 = current_adata[current_adata.obs_names.isin(idx1)]\n",
    "    embd0 = embeddings_df.loc[idx0].values\n",
    "    embd1 = embeddings_df.loc[idx1].values\n",
    "    print(\"Recursing on child 0 with\", adata0.X.shape[0], \"voxels\")\n",
    "    child0 = _dosplit(adata0, embd0, path + [0], splitlevel + 1)\n",
    "    print(\"Recursing on child 1 with\", adata1.X.shape[0], \"voxels\")\n",
    "    child1 = _dosplit(adata1, embd1, path + [1], splitlevel + 1)\n",
    "    node.children[0] = child0\n",
    "    node.children[1] = child1\n",
    "    print(\"Completed _dosplit at level\", splitlevel)\n",
    "    return node\n",
    "\n",
    "# Start the recursive clustering from the root\n",
    "print(\"\\n=== Starting recursive clustering ===\")\n",
    "root_node = _dosplit(adata[::ds_factor], standardized_embeddings_GLOBAL[::ds_factor].values, path=[], splitlevel=0)\n",
    "print(\"Recursive clustering complete.\")\n",
    "\n",
    "# Save the clustering log and tree to file.\n",
    "clusteringLOG.to_parquet(\"tree_clustering.parquet\")\n",
    "with open(\"rootnode_clustering.pkl\", \"wb\") as f:\n",
    "    pickle.dump(root_node, f)\n",
    "print(\"Clustering log and root node saved to file.\")\n",
    "\n",
    "return root_node, clusteringLOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abd33c-cccb-41dd-8a9d-05df8d24d46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
